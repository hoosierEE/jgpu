Hardware configurations used for testing.

* Dell Desktop
 - cpu: Intel Core 2 Quad
 - ram: 8GB
 - os: Ubuntu 20.04
 - nickname: "white dell"
** gpu
Nvidia GeForce 9800GT

Specifications below copied from [[https://www.techpowerup.com/gpu-specs/geforce-9800-gt.c635][this link]]:

*** features
- DirectX: 11.1 (10_0)
- OpenGL: 3.3
- OpenCL: 1.1
- Vulkan: N/A
- CUDA: 1.1
- Shader Model: 4.0

*** clock speeds
- gpu clock: 600 MHz
- shader clock: 1500 MHz
- memory clock: 900 MHz, 1800 Mbps effective

*** memory
- size: 512 MB
- type: GDDR3
- bus: 256 bit
- bandwidth: 57.6 GB/s

*** render config
- shading units: 112
- TMUs: 56
- ROPs: 16
- SM count: 14
- L2 Cache: 64KB

*** theoretical performance
- pixel rate: 9.600 GPixel/s
- texture rate: 33.60 GTexel/s
- FP32 (float) performance: 336.0 GFLOPS

*** relative performance
State-of-the-art GeForce RTX 3090 is about 30.54x faster than this card.

* Macbook Pro
- cpu: 2.2 GHz Intel Core i7
- ram: 16GB 1600MHz DDR3
- gpu: Intel Iris Pro 1536 MB (integrated graphics)
- os: macOS Mojave (10.14.6)
- nickname: "ashroyer-mbp"
