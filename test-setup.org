Hardware configurations used for testing.

* Dell Desktop
 - cpu: Intel Core 2 Quad
 - ram: 8GB
 - os: Ubuntu 20.04
 - nickname: "white dell"
** gpu
Nvidia GeForce 9500GT

Specifications below copied from [[https://www.techpowerup.com/gpu-specs/geforce-9500-gt.c3373][this link]]:

*** features
- DirectX: 11.1 (10_0)
- OpenGL: 3.3
- OpenCL: 1.1
- Vulkan: N/A
- CUDA: 1.1
- Shader Model: 4.0

*** clock speeds
- gpu clock: 600 MHz
- shader clock: 1500 MHz
- memory clock: 1000 MHz, 2000 Mbps effective

*** memory
- size: 512 MB
- type: GDDR3
- bus: 128 bit
- bandwidth: 32.0 GB/s

*** render config
- shading units: 32
- TMUs: 16
- ROPs: 8
- SM count: 4
- L2 Cache: 32KB

*** theoretical performance
- pixel rate: 4.8 GPixel/s
- texture rate: 9.6 GTexel/s
- FP32 (float) performance: 96.0 GFLOPS

*** relative performance
State-of-the-art GeForce RTX 3090 is about 90x faster than this card.

* Macbook Pro
- cpu: 2.2 GHz Intel Core i7
- ram: 16GB 1600MHz DDR3
- gpu: Intel Iris Pro 1536 MB (integrated graphics)
- os: macOS Mojave (10.14.6)
- nickname: "ashroyer-mbp"
