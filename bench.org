* Matrix Multiply Benchmarks
1. measure performance of matmul for various matrix sizes on GPU
2. determine break-even data size (if any) where GPU is faster

For simplicity, these experiments will consider only square matrices of edge length N, in this form:
Example (N=3):
#+begin_example
0 1 2
3 4 5
6 7 8
#+end_example
Another example (N=5):
#+begin_example
 0  1  2  3  4
 5  6  7  8  9
10 11 12 13 14
15 16 17 18 19
20 21 22 23 24
#+end_example

The ~matmul~ operation will be timed with two identical such matrices as its arguments.

* Results
All timing measurements are captured within J, and show the duration of the matrix multiply operation only.

#+name: n_timings
|     N |       CPU | GPU (cold) | GPU (warm) |
|-------+-----------+------------+------------|
|    10 |   0.00003 |    0.00149 |    0.00003 |
|   100 |   0.00028 |    0.00157 |    0.00003 |
|  1000 |   0.07671 |    0.00172 |    0.00012 |
|  2000 |   0.43586 |    0.00265 |    0.00014 |
|  4000 |   3.43190 |    0.00635 |    0.00025 |
|  5000 |   6.48677 |    0.00909 |    0.00036 |
|  6000 |  10.99981 |    0.01261 |    0.00047 |
|  8000 |  25.81416 |    0.02156 |    0.00068 |
| 10000 |  50.19670 |    0.03276 |    0.00106 |
| 15000 | 174.86698 |    0.03281 |    0.00213 |


This does not capture the whole story, because in the real world, transferring data between CPU and GPU memory takes significant time, whereas if data acquisition, computation, and output all occur on the CPU, there are no extra transfers.

It is true that CPU memory is non-uniform, and consists of fast registers, fairly fast caches, and slower main memory.
But when using a GPU as an accelerator, the total memory latency suffers from all of these regular effects, plus the overhead of transferring to the GPU's memory.
